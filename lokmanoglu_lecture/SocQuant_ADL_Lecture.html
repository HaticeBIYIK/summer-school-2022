<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Topic Modeling in R Studio</title>
    <meta charset="utf-8" />
    <meta name="author" content="Ayse Deniz Lokmanoglu" />
    <meta name="date" content="2022-07-26" />
    <script src="SocQuant_ADL_Lecture_files/header-attrs/header-attrs.js"></script>
    <script src="SocQuant_ADL_Lecture_files/clipboard/clipboard.min.js"></script>
    <link href="SocQuant_ADL_Lecture_files/xaringanExtra-clipboard/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="SocQuant_ADL_Lecture_files/xaringanExtra-clipboard/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"Copy","success":"Copied!","error":"Press Ctrl+C to Copy"})</script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Topic Modeling in R Studio
]
.subtitle[
## 2nd Summer School in Computational Social Sciences
]
.author[
### Ayse Deniz Lokmanoglu
]
.date[
### July 26, 2022
]

---






## What is text analysis?
* Deriving information from a text
* When we have a lot of text, computational methods help us derive detailed information from the text faster, and with less researcher bias
* Today we will learn how to do topic modeling using R Studio
---

## What is Topic Modeling?
* Topic modeling is a bag-of-words approach
* Topic modeling, or Latent Dirichlet Allocation (LDA), is a computational content analysis tool that surfaces the "hidden thematic structure of a collection of text" (Maier et al., 2018: 93)
* Through an inductive approach to quantitative measurements, it allows researchers to conduct semantic analysis on a large number of texts. 
* LDA conducts measurements in three levels: corpus, documents, and terms. The corpus consists of a collection of documents, and each document consists of a collection of words (referred to as terms in the algorithm). 
* The LDA algorithm models the representation of the words, with each other, within a document and within the corpus, through "topics" (Maier et al., 2018: 94). 
* These facilitate researchers to label topics inductively, by using both the words within each topic, as well as the documents in each topic. 
* Thus, LDA analysis allows a document to represent multiple topics, providing a deeper insight into the thematic structure of the corpus.
---
[![image.png](https://i.postimg.cc/6591RPdk/image.png)](https://postimg.cc/SjvrbD1d)
---
## Let's start coding!
First we install all the packages! You only have to do this once, for all other times you just need to load the packages (see next slide)
.small-code[

```r
# this code is to install all the packages, you only need to run this once, afterwards all you need is to load the packages 
install.packages(c(
  "tidyverse",   # foundation packages needed for text analysis
  "tidytext",    # foundation packages needed for text analysis
  "dplyr",       # foundation packages needed for text analysis
  "tm",          # text mining package   
  "quanteda",    # Quantitative Analysis of Textual Data
  "ldatuning",   # Tuning of the Latent Dirichlet Allocation Models Parameters
  "topicmodels", # Topic Model package
  "scales",      # scale functions for visualizations
  "ggthemes",    # graph theme options
  "jtools",      # ggplot2 themes
  "ggplot2",     # visualization
  "lubridate",   # for dates
  "zoo"          # another package for dates
  ))
```
]

---
### Load packages
.small-code[

```r
library(tidyverse)
library(tidytext)
library(dplyr)
library(tm)
library(quanteda)
library(ldatuning)
library(topicmodels)
library(scales)
library(ggthemes)
library(lubridate)
library(jtools)
```
]
How to find details on packages?
Type the package name preceded by ? and you will see the package details on the help window
.small-code[

```r
?tidyverse
```
]
---
### Our dataset
.small-code[

```r
url&lt;-c("https://dataverse.harvard.edu/api/access/datafile/6389385")
mydata &lt;- read_csv(url)
glimpse(mydata)
```

```
## Rows: 1,500
## Columns: 5
## $ ...1          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1~
## $ index         &lt;dbl&gt; 98474, 23319, 144569, 38059, 97919, 131938, 21368, 16530~
## $ date          &lt;date&gt; 2021-11-21, 2021-07-29, 2021-04-16, 2021-08-26, 2021-11~
## $ source.domain &lt;chr&gt; "foxnews.com", "foxnews.com", "dailywire.com", "abc13.co~
## $ originaltext  &lt;chr&gt; "chicago mayor needs to dump police boss if &lt;U+0091&gt;crim~
```
]
---
As you can see we have an index column from csv labelled *...1*, and date column. So first lets remove the extra column, make sure date is coded as date.
* Why did we write the command `select()` with its package name?
.small-code[

```r
mydata &lt;- mydata %&gt;%
  dplyr::select(-...1) %&gt;% #from dplyr we drop the column with -, and this case 
  mutate(date=ymd(date)) %&gt;% #using lubridate we change date column to date variable
  mutate(text=originaltext) #Create a new column labelled text - to keep original text safe
glimpse(mydata) #lets see what our dataset is made of
```

```
## Rows: 1,500
## Columns: 5
## $ index         &lt;dbl&gt; 98474, 23319, 144569, 38059, 97919, 131938, 21368, 16530~
## $ date          &lt;date&gt; 2021-11-21, 2021-07-29, 2021-04-16, 2021-08-26, 2021-11~
## $ source.domain &lt;chr&gt; "foxnews.com", "foxnews.com", "dailywire.com", "abc13.co~
## $ originaltext  &lt;chr&gt; "chicago mayor needs to dump police boss if &lt;U+0091&gt;crim~
## $ text          &lt;chr&gt; "chicago mayor needs to dump police boss if &lt;U+0091&gt;crim~
```
]
---
### Preprocessing, getting ready for LDA
* Tokenize it
.tiny-code[

```r
toks &lt;- tokens(mydata$text,
               remove_punct = TRUE,
               remove_symbols = TRUE,
               remove_numbers = TRUE,
               remove_url = TRUE,
               remove_separators = TRUE,
               split_hyphens = FALSE,
               include_docvars = TRUE,
               padding = FALSE) %&gt;%
  tokens_remove(stopwords(language = "en")) %&gt;% #for this we used combined stopwords list from google with quanteda 
  tokens_select(min_nchar = 2)
head(toks)
```

```
## Tokens consisting of 6 documents.
## text1 :
##  [1] "chicago"   "mayor"     "needs"     "dump"      "police"    "boss"     
##  [7] "crime"     "pandemic"  "isn"       "addressed" "critic"    "says"     
## [ ... and 17 more ]
## 
## text2 :
##  [1] "randi"      "weingarten" "ripped"     "telling"    "msnbc"     
##  [6] "going"      "try"        "reopen"     "schools"    "cdc"       
## [11] "mask"       "guidance"  
## [ ... and 17 more ]
## 
## text3 :
##  [1] "pfizer"  "ceo"     "third"   "covid"   "vaccine" "dose"    "likely" 
##  [8] "needed"  "within"  "months"  "pfizer"  "ceo"    
## [ ... and 22 more ]
## 
## text4 :
##  [1] "texas"        "researchers"  "develop"      "treatment"    "help"        
##  [6] "fight"        "covid-19"     "embed"        "news"         "videos"      
## [11] "fda-approved" "drug"        
## [ ... and 11 more ]
## 
## text5 :
##  [1] "nyc"          "civil"        "service"      "exam"         "applications"
##  [6] "open"         "december"     "staten"       "island"       "n.y"         
## [11] "new"          "york"        
## [ ... and 13 more ]
## 
## text6 :
##  [1] "american"  "united"    "airlines"  "cancel"    "furloughs" "covid-19" 
##  [7] "relief"    "bill"      "passes"    "two"       "biggest"   "u.s"      
## [ ... and 12 more ]
```
]
---
### Next Steps
* Change it into a [document-feature matrix](https://quanteda.io/reference/dfm.html)
* Match your dfm object with your original data frame through index
.small-code[

```r
dfm_counts&lt;- dfm(toks) 
rm(toks) 
docnames(dfm_counts)&lt;-mydata$index#remove unused files to save space
```
]
---
### LDA Object
* Convert dfm object to an LDA object
.small.code[

```r
dtm_lda &lt;- convert(dfm_counts, to = "topicmodels",docvars = dfm_counts@docvars) #convert the data set to a document term matrix
n &lt;- nrow(dtm_lda) # number of rows for cross-validation method
rm(dfm_counts) # remove for space
dtm_lda
```

```
## &lt;&lt;DocumentTermMatrix (documents: 1500, terms: 9225)&gt;&gt;
## Non-/sparse entries: 38025/13799475
## Sparsity           : 100%
## Maximal term length: 22
## Weighting          : term frequency (tf)
```
]
---

class: center, inverse, middle

# Let's run our topic model!

---
### Find K
* This function is from [ldatuning](https://cran.r-project.org/web/packages/ldatuning/vignettes/topics.html) package
**I ran the code already to save time** *You can run it on your own time by erasing the markdown option 'eval=FALSE'* 
.small-code[

```r
Sys.time()
result &lt;- FindTopicsNumber(
  dtm_lda,
  topics = seq(2,50,by=10), # Specify how many topics you want to try.
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 9), # random seed number
  mc.cores = 2L,
  verbose = TRUE
)
Sys.time()
save(result, file="Class_FindK.Rda")
FindTopicsNumber_plot(result)
ggsave("Class_Find_K.jpg", width=8.5, height=5, dpi=150)
```
]

---
### Plot Result
[![Class-Find-K.jpg](https://i.postimg.cc/dtPr1JNq/Class-Find-K.jpg)](https://postimg.cc/SjdJ1bp5)

---

### Let's run our topic model!
We identified our optimal k as 22 from the graph, but for our ease of analysis we will model on 5 topics
.small-code[

```r
Sys.time()
```

```
## [1] "2022-07-21 20:47:35 CDT"
```

```r
covid_lda &lt;- LDA(dtm_lda, k = 5, control = list(seed = 1234))
save(covid_lda, file="Class_lda_K5.Rda") #always save your variables
Sys.time()
```

```
## [1] "2022-07-21 20:47:44 CDT"
```

```r
covid_lda
```

```
## A LDA_VEM topic model with 5 topics.
```
]
---

### Extract data from the lda model
* We can extract top words and documents
.small-code[

```r
covid_topics &lt;- tidy(covid_lda, matrix = "beta")
head(covid_topics)
```

```
## # A tibble: 6 x 3
##   topic term        beta
##   &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;
## 1     1 chicago 6.16e- 4
## 2     2 chicago 5.42e- 4
## 3     3 chicago 1.10e- 4
## 4     4 chicago 6.74e-12
## 5     5 chicago 1.13e-26
## 6     1 mayor   1.04e- 3
```
]

---

### Visualize top words
.small-code[

```r
covid_top_terms &lt;- covid_topics %&gt;%
  group_by(topic) %&gt;%
  slice_max(beta, n = 10) %&gt;% 
  ungroup() %&gt;%
  arrange(topic, -beta)
```
&lt;img src="SocQuant_ADL_Lecture_files/figure-html/topterms-plot-1.svg" width="100%" /&gt;
]

---

### We can label topics using top words
* In our case it looks like
  + Topic 1: Variants
  + Topic 2: Vaccine Mandates
  + Topic 3: Pandemic Regulations
  + Topic 4: Relief Stimulus
  + Topic 5: Covid19 and Government
* We should create a variable called topic_names and save it for future
.small-code[

```r
topic_names&lt;-c("Variants", "Vaccine_Mandates", "Pandemic_Regulations", "Relief_Stimulus", "Covid19_and_Government")
```
]
***Why did I use underscore when creating the `topic_names` variable?*** 

---

### Document-topic probabilities
* We will extract the γ (“gamma”) value which is per-document-per-topic probabilities.This value estimates the proportion of words from each document that belong to that topic.  
.small-code[

```r
covid_documents &lt;- tidy(covid_lda, matrix = "gamma")
glimpse(covid_documents)
```

```
## Rows: 7,500
## Columns: 3
## $ document &lt;chr&gt; "98474", "23319", "144569", "38059", "97919", "131938", "2136~
## $ topic    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~
## $ gamma    &lt;dbl&gt; 0.2794112292, 0.0016247089, 0.0013874387, 0.7576687978, 0.001~
```
]

---

### Join with original document?
* We saw in our gamma values we have a document number **equal to our index** (from previous slides). We can join with our document and see for example which topics come from what domains?
* But first we can see that the dimensions of `covid_documents` and `mydata` are different, why?
.small-code[

```r
dim(covid_documents)
```

```
## [1] 7500    3
```

```r
dim(mydata)
```

```
## [1] 1500    5
```
]

---

### Wide documents
* What we have is a long document, What we need to do is change the document to wider, having each document with topics as columns
* We can reshape the data frame using`dpylr`'s `pivot_wider()` and `pivot_longer()`
.tiny.code[

```r
?pivot_wider()
```
]
.small-code[

```r
covid_documents_wide&lt;- covid_documents %&gt;%
  pivot_wider(names_from = topic,
              values_from = gamma)
dim(covid_documents_wide)
```

```
## [1] 1500    6
```

```r
dim(mydata)
```

```
## [1] 1500    5
```
]

---
### Column Names - Good Practice
* It is good practice to not have numbers as column names, so let's add a prefix of X
.tiny-code[

```r
colnames(covid_documents_wide)[2:6] &lt;- paste("X", colnames(covid_documents_wide[,c(2:6)]), sep = "_")
colnames(covid_documents_wide)
```

```
## [1] "document" "X_1"      "X_2"      "X_3"      "X_4"      "X_5"
```
]
* We can also add our topic_names the same way
.tiny-code[

```r
covid_documents_wide_test&lt;-covid_documents_wide # to save a backup copy
colnames(covid_documents_wide_test)[2:6] &lt;- topic_names
colnames(covid_documents_wide_test)
```

```
## [1] "document"               "Variants"               "Vaccine_Mandates"      
## [4] "Pandemic_Regulations"   "Relief_Stimulus"        "Covid19_and_Government"
```
]

---

### Now let's join!
.tiny-code[

```r
meta_theta_df&lt;-left_join(mydata, covid_documents_wide, by=c("index" = "document"))
```

```
## Error in `left_join()`:
## ! Can't join on `x$index` x `y$index` because of incompatible types.
## i `x$index` is of type &lt;double&gt;&gt;.
## i `y$index` is of type &lt;character&gt;&gt;.
```
]
We need to change document in covid_documents_wide to number
.tiny-code[

```r
covid_documents_wide &lt;- covid_documents_wide %&gt;%
  mutate(document=as.numeric(document))
typeof(covid_documents_wide$document) # this is a way to check the type 
```

```
## [1] "double"
```
]
Let's try again!
.tiny-code[

```r
meta_theta_df&lt;-left_join(mydata, covid_documents_wide, by=c("index" = "document"))
meta_theta_df
```

```
## # A tibble: 1,500 x 10
##     index date       source.domain    originaltext text      X_1     X_2     X_3
##     &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;            &lt;chr&gt;        &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
##  1  98474 2021-11-21 foxnews.com      chicago may~ chic~ 0.279   0.716   0.00162
##  2  23319 2021-07-29 foxnews.com      randi weing~ rand~ 0.00162 0.00162 0.894  
##  3 144569 2021-04-16 dailywire.com    pfizer ceo:~ pfiz~ 0.00139 0.00139 0.00139
##  4  38059 2021-08-26 abc13.com        texas a&amp;m r~ texa~ 0.758   0.174   0.0638 
##  5  97919 2021-11-28 silive.com       nyc civil s~ nyc ~ 0.00188 0.992   0.00188
##  6 131938 2021-03-12 fox13news.com    american, u~ amer~ 0.00196 0.00196 0.00196
##  7  21368 2021-07-23 huffpost.com     ted cruz&lt;U+~ ted ~ 0.00143 0.00143 0.00143
##  8 165309 2021-05-17 cnn.com          analysis: a~ anal~ 0.201   0.566   0.230  
##  9  25010 2021-07-14 patriotproject.~ az senate p~ az s~ 0.00162 0.00162 0.609  
## 10  76043 2021-10-25 reuters.com      u.s. to out~ u.s.~ 0.00121 0.213   0.00121
## # ... with 1,490 more rows, and 2 more variables: X_4 &lt;dbl&gt;, X_5 &lt;dbl&gt;
```
]

---

### Let's look at the domains
.tiny-code[

```r
domains &lt;- meta_theta_df %&gt;%
  dplyr::select(source.domain, X_1:X_5) %&gt;% # selected domains and topic gammas for each document
  group_by(source.domain) %&gt;% # grouped by domains 
  summarise(across(everything(), sum)) # summed all the topic gammas
dim(domains)
```

```
## [1] 472   6
```
]
***Now you can see we have a new data-set with 472 domains, and the topic probabilities for each domain***

---

### Which domain has the highest topic probabilities?
* let's do topic 4 and the top 10 domains
.small-code[

```r
topic4 &lt;- domains %&gt;%
  slice_max(X_4, n=10)
head(topic4)
```

```
## # A tibble: 6 x 6
##   source.domain        X_1   X_2   X_3   X_4   X_5
##   &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 nbcnews.com         9.06  4.84  9.04 11.8  10.3 
## 2 nytimes.com         9.95 10.5   6.08  7.92  7.55
## 3 cnn.com            11.1   6.81  7.93  7.75 14.4 
## 4 washingtonpost.com  5.56  6.33 12.7   6.79  2.66
## 5 cnbc.com            5.15  1.61  2.99  6.55  6.71
## 6 dailywire.com       8.04  4.10  6.15  5.69  6.02
```
]
* play with different topics and `slice_max()` &amp; `slice_min()`from `dplyr` [package](https://dplyr.tidyverse.org/reference/slice.html)

---

### Visualize comparison of domains
* Let's compare cnn.com and nbcnews.com
  + We want to make a graph bar graph that has both domains and topic probabilities. 
  + First let's create a smaller dataframe with the two domains

.tiny-code[

```r
domain_comp&lt;- domains %&gt;%
  filter(source.domain=="cnn.com" | source.domain=="nbcnews.com")
domain_comp
```

```
## # A tibble: 2 x 6
##   source.domain   X_1   X_2   X_3   X_4   X_5
##   &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 cnn.com       11.1   6.81  7.93  7.75  14.4
## 2 nbcnews.com    9.06  4.84  9.04 11.8   10.3
```
]

---

### Set the data for bar graph
* We need to make domains a group, topics as x axis and gamma values as y.
* So we need to make the document long, by `dplyr` packages `pivot_longer()`
.tiny-code[

```r
domain_long &lt;- domain_comp %&gt;%
  pivot_longer(!source.domain,
               names_to = "topics", # names as topic
               values_to = "gamma")
head(domain_long)
```

```
## # A tibble: 6 x 3
##   source.domain topics gamma
##   &lt;chr&gt;         &lt;chr&gt;  &lt;dbl&gt;
## 1 cnn.com       X_1    11.1 
## 2 cnn.com       X_2     6.81
## 3 cnn.com       X_3     7.93
## 4 cnn.com       X_4     7.75
## 5 cnn.com       X_5    14.4 
## 6 nbcnews.com   X_1     9.06
```
]

---

### Now lets graph it
.small-code[
&lt;img src="SocQuant_ADL_Lecture_files/figure-html/domain-graph-1.svg" width="100%" /&gt;
]

---

### Play with graphs
* We can also add our topic labels, play styles using `ggthemes()` package
.small-code[
&lt;img src="SocQuant_ADL_Lecture_files/figure-html/domain-graph-fancy-1.svg" width="100%" /&gt;
]

---

### Lastly let's do topics over time
* For this we well again use our meta_theta_df document, this time we will summarize by dates
.small-code[

```r
topics_time &lt;- meta_theta_df %&gt;%
  dplyr::select(date, X_1:X_5) %&gt;% # selected dates and topic gammas for each document
  group_by(date) %&gt;% # grouped by dates 
  summarise(across(everything(), mean)) # summed all the topic gammas
```
]
* We have to make this document long to plot it, using `pivot_longer()`
.tiny-code[

```r
topic_time_long &lt;- topics_time %&gt;%
  pivot_longer(!date, # long from date
               names_to = "topics", # names as topic
               values_to = "gamma") # values as gamma
topic_time_long
```

```
## # A tibble: 1,785 x 3
##    date       topics   gamma
##    &lt;date&gt;     &lt;chr&gt;    &lt;dbl&gt;
##  1 2021-01-01 X_1    0.138  
##  2 2021-01-01 X_2    0.135  
##  3 2021-01-01 X_3    0.0345 
##  4 2021-01-01 X_4    0.266  
##  5 2021-01-01 X_5    0.427  
##  6 2021-01-02 X_1    0.00309
##  7 2021-01-02 X_2    0.163  
##  8 2021-01-02 X_3    0.336  
##  9 2021-01-02 X_4    0.495  
## 10 2021-01-02 X_5    0.00309
## # ... with 1,775 more rows
```
]

---

### Visualize it
.small-code[
&lt;img src="SocQuant_ADL_Lecture_files/figure-html/topictime-plot-1.svg" width="100%" /&gt;
]
* This is very crowded 

---

### Simpler graph
* let's pick topics 2 and 4
.small-code[
&lt;img src="SocQuant_ADL_Lecture_files/figure-html/topictime-plot-2-1.svg" width="100%" /&gt;
]

---

### Even simpler graph
* Make it monthly
.small-code[

```r
meta_theta_df$yearmonth&lt;-my(zoo::as.yearmon(meta_theta_df$date))
topics_year_mon &lt;- meta_theta_df %&gt;%
  dplyr::select(yearmonth, X_1:X_5) %&gt;% # selected dates and topic gammas for each document
  group_by(yearmonth) %&gt;% # grouped by dates 
  summarise(across(everything(), mean))
# pivot longer like before
topics_year_mon_long &lt;- topics_year_mon %&gt;%
  pivot_longer(!yearmonth, # long from date
               names_to = "topics", # names as topic
               values_to = "gamma") # values as gamma
head(topics_year_mon_long)
```

```
## # A tibble: 6 x 3
##   yearmonth  topics gamma
##   &lt;date&gt;     &lt;chr&gt;  &lt;dbl&gt;
## 1 2021-01-01 X_1    0.230
## 2 2021-01-01 X_2    0.192
## 3 2021-01-01 X_3    0.125
## 4 2021-01-01 X_4    0.227
## 5 2021-01-01 X_5    0.226
## 6 2021-02-01 X_1    0.180
```
]

---
### Lets graph it again
* Mean monthly topic probabilities
.small-code[
&lt;img src="SocQuant_ADL_Lecture_files/figure-html/topicmonth-plot-1.svg" width="100%" /&gt;
]

---

class: center, inverse, middle

# Questions?

---

class: center, inverse, middle

# Thank you!
My email is ayse.lokmanoglu@northwestern.edu and my github page where I have more challenging topic model codes to play with!

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current%",
"highlightStyle": "github",
"highlightLines": true,
"ratio": "3:2",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
